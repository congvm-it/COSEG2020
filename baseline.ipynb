{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "\n",
    "import monai\n",
    "from monai.handlers import CheckpointSaver, MeanDice, StatsHandler, ValidationHandler\n",
    "from monai.transforms import (\n",
    "    AddChanneld,\n",
    "    CastToTyped,\n",
    "    LoadNiftid,\n",
    "    Orientationd,\n",
    "    RandAffined,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandFlipd,\n",
    "    RandGaussianNoised,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    SpatialPadd,\n",
    "    ToTensord,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xforms(mode=\"train\", keys=(\"image\", \"label\")):\n",
    "    \"\"\"returns a composed transform for train/val/infer.\"\"\"\n",
    "\n",
    "    xforms = [\n",
    "        LoadNiftid(keys),\n",
    "        AddChanneld(keys),\n",
    "        Orientationd(keys, axcodes=\"LPS\"),\n",
    "        Spacingd(keys, pixdim=(1.25, 1.25, 5.0), mode=(\"bilinear\", \"nearest\")[: len(keys)]),\n",
    "        ScaleIntensityRanged(keys[0], a_min=-1000.0, a_max=500.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "    ]\n",
    "    if mode == \"train\":\n",
    "        xforms.extend(\n",
    "            [\n",
    "                SpatialPadd(keys, spatial_size=(192, 192, -1), mode=\"reflect\"),  # ensure at least 192x192\n",
    "                RandAffined(\n",
    "                    keys,\n",
    "                    prob=0.15,\n",
    "                    rotate_range=(-0.05, 0.05),\n",
    "                    scale_range=(-0.1, 0.1),\n",
    "                    mode=(\"bilinear\", \"nearest\"),\n",
    "                    as_tensor_output=False,\n",
    "                ),\n",
    "                RandCropByPosNegLabeld(keys, label_key=keys[1], spatial_size=(192, 192, 16), num_samples=3),\n",
    "                RandGaussianNoised(keys[0], prob=0.15, std=0.01),\n",
    "                RandFlipd(keys, spatial_axis=0, prob=0.5),\n",
    "                RandFlipd(keys, spatial_axis=1, prob=0.5),\n",
    "                RandFlipd(keys, spatial_axis=2, prob=0.5),\n",
    "            ]\n",
    "        )\n",
    "        dtype = (np.float32, np.uint8)\n",
    "    if mode == \"val\":\n",
    "        dtype = (np.float32, np.uint8)\n",
    "    if mode == \"infer\":\n",
    "        dtype = (np.float32,)\n",
    "    xforms.extend([CastToTyped(keys, dtype=dtype), ToTensord(keys)])\n",
    "    return monai.transforms.Compose(xforms)\n",
    "\n",
    "\n",
    "def get_net():\n",
    "    \"\"\"returns a unet model instance.\"\"\"\n",
    "\n",
    "    n_classes = 2\n",
    "    net = monai.networks.nets.BasicUNet(\n",
    "        dimensions=3,\n",
    "        in_channels=1,\n",
    "        out_channels=n_classes,\n",
    "        features=(32, 32, 64, 128, 256, 32),\n",
    "        dropout=0.1,\n",
    "    )\n",
    "    return net\n",
    "\n",
    "\n",
    "def get_inferer(_mode=None):\n",
    "    \"\"\"returns a sliding window inference instance.\"\"\"\n",
    "\n",
    "    patch_size = (192, 192, 16)\n",
    "    sw_batch_size, overlap = 2, 0.5\n",
    "    inferer = monai.inferers.SlidingWindowInferer(\n",
    "        roi_size=patch_size,\n",
    "        sw_batch_size=sw_batch_size,\n",
    "        overlap=overlap,\n",
    "        mode=\"gaussian\",\n",
    "        padding_mode=\"replicate\",\n",
    "    )\n",
    "    return inferer\n",
    "\n",
    "\n",
    "class DiceCELoss(nn.Module):\n",
    "    \"\"\"Dice and Xentropy loss\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dice = monai.losses.DiceLoss(to_onehot_y=True, softmax=True)\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        dice = self.dice(y_pred, y_true)\n",
    "        # CrossEntropyLoss target needs to have shape (B, D, H, W)\n",
    "        # Target from pipeline has shape (B, 1, D, H, W)\n",
    "        cross_entropy = self.cross_entropy(y_pred, torch.squeeze(y_true, dim=1).long())\n",
    "        return dice + cross_entropy\n",
    "\n",
    "\n",
    "def train(data_folder=\".\", \n",
    "          model_folder=\"runs\", \n",
    "          num_workers=8, \n",
    "          preprocessing_workers=4,\n",
    "          batch_size=32):\n",
    "    \"\"\"run a training pipeline.\"\"\"\n",
    "\n",
    "    images = sorted(glob.glob(os.path.join(data_folder, \"*_ct.nii.gz\")))\n",
    "    labels = sorted(glob.glob(os.path.join(data_folder, \"*_seg.nii.gz\")))\n",
    "    logging.info(f\"training: image/label ({len(images)}) folder: {data_folder}\")\n",
    "\n",
    "    amp = True  # auto. mixed precision\n",
    "    keys = (\"image\", \"label\")\n",
    "    \n",
    "    #TODO\n",
    "    is_one_hot = False  # whether the label has multiple channels to represent  multiple class\n",
    "    \n",
    "    train_frac, val_frac = 0.8, 0.2\n",
    "    n_train = int(train_frac * len(images)) + 1\n",
    "    n_val = min(len(images) - n_train, int(val_frac * len(images)))\n",
    "    logging.info(f\"training: train {n_train} val {n_val}, folder: {data_folder}\")\n",
    "\n",
    "    train_files = [{keys[0]: img, keys[1]: seg} for img, seg in zip(images[:n_train], labels[:n_train])]\n",
    "    val_files = [{keys[0]: img, keys[1]: seg} for img, seg in zip(images[-n_val:], labels[-n_val:])]\n",
    "\n",
    "    # create a training data loader\n",
    "    logging.info(f\"batch size {batch_size}\")\n",
    "    train_transforms = get_xforms(\"train\", keys)\n",
    "    train_ds = monai.data.CacheDataset(data=train_files,\n",
    "                                       cache_rate=0.5,\n",
    "                                       transform=train_transforms, \n",
    "                                       num_workers=preprocessing_workers)\n",
    "    train_loader = monai.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    # create a validation data loader\n",
    "    val_transforms = get_xforms(\"val\", keys)\n",
    "    val_ds = monai.data.CacheDataset(data=val_files, \n",
    "                                     cache_rate=0.1,\n",
    "                                     transform=val_transforms)\n",
    "    val_loader = monai.data.DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=1,  # image-level batch to the sliding window method, not the window-level batch\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    # create BasicUNet, DiceLoss and Adam optimizer\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net = get_net().to(device)\n",
    "    max_epochs, lr, momentum = 500, 1e-4, 0.95\n",
    "    logging.info(f\"epochs {max_epochs}, lr {lr}, momentum {momentum}\")\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    # create evaluator (to be used to measure model quality during training\n",
    "#     val_metric = MeanDice(\n",
    "#         include_background=False,\n",
    "#         to_onehot_y=not is_one_hot,\n",
    "#         mutually_exclusive=True,\n",
    "#         output_transform=lambda x: (x[\"pred\"], x[\"label\"]),\n",
    "#     )\n",
    "    \n",
    "    val_metric = MeanDice(\n",
    "        include_background=False,\n",
    "        device = device,\n",
    "        output_transform=lambda x: (x[\"pred\"], x[\"label\"]),\n",
    "    )\n",
    "    \n",
    "    val_handlers = [\n",
    "        ProgressBar(),\n",
    "        CheckpointSaver(save_dir=model_folder, save_dict={\"net\": net}, save_key_metric=True, key_metric_n_saved=3),\n",
    "    ]\n",
    "    evaluator = monai.engines.SupervisedEvaluator(\n",
    "        device=device,\n",
    "        val_data_loader=val_loader,\n",
    "        network=net,\n",
    "        inferer=get_inferer(),\n",
    "        key_val_metric={\"val_mean_dice\": val_metric},\n",
    "        val_handlers=val_handlers,\n",
    "        amp=amp,\n",
    "    )\n",
    "\n",
    "    # evaluator as an event handler of the trainer\n",
    "    train_handlers = [\n",
    "        ValidationHandler(validator=evaluator, interval=1, epoch_level=True),\n",
    "        StatsHandler(tag_name=\"train_loss\", output_transform=lambda x: x[\"loss\"]),\n",
    "    ]\n",
    "    trainer = monai.engines.SupervisedTrainer(\n",
    "        device=device,\n",
    "        max_epochs=max_epochs,\n",
    "        train_data_loader=train_loader,\n",
    "        network=net,\n",
    "        optimizer=opt,\n",
    "        loss_function=DiceCELoss(),\n",
    "        inferer=get_inferer(),\n",
    "        key_train_metric=None,\n",
    "        train_handlers=train_handlers,\n",
    "        amp=amp,\n",
    "    )\n",
    "    trainer.run()\n",
    "\n",
    "\n",
    "def infer(data_folder=\".\", model_folder=\"runs\", prediction_folder=\"output\"):\n",
    "    \"\"\"\n",
    "    run inference, the output folder will be \"./output\"\n",
    "    \"\"\"\n",
    "    ckpts = sorted(glob.glob(os.path.join(model_folder, \"*.pt\")))\n",
    "    ckpt = ckpts[-1]\n",
    "    for x in ckpts:\n",
    "        logging.info(f\"available model file: {x}.\")\n",
    "    logging.info(\"----\")\n",
    "    logging.info(f\"using {ckpt}.\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net = get_net().to(device)\n",
    "    net.load_state_dict(torch.load(ckpt, map_location=device))\n",
    "    net.eval()\n",
    "\n",
    "    image_folder = os.path.abspath(data_folder)\n",
    "    images = sorted(glob.glob(os.path.join(image_folder, \"*_ct.nii.gz\")))\n",
    "    logging.info(f\"infer: image ({len(images)}) folder: {data_folder}\")\n",
    "    infer_files = [{\"image\": img} for img in images]\n",
    "\n",
    "    keys = (\"image\",)\n",
    "    infer_transforms = get_xforms(\"infer\", keys)\n",
    "    infer_ds = monai.data.Dataset(data=infer_files, transform=infer_transforms)\n",
    "    infer_loader = monai.data.DataLoader(\n",
    "        infer_ds,\n",
    "        batch_size=1,  # image-level batch to the sliding window method, not the window-level batch\n",
    "        num_workers=2,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    inferer = get_inferer()\n",
    "    saver = monai.data.NiftiSaver(output_dir=prediction_folder, mode=\"nearest\")\n",
    "    with torch.no_grad():\n",
    "        for infer_data in infer_loader:\n",
    "            logging.info(f\"segmenting {infer_data['image_meta_dict']['filename_or_obj']}\")\n",
    "            preds = inferer(infer_data[keys[0]].to(device), net)\n",
    "            n = 1.0\n",
    "            for _ in range(4):\n",
    "                # test time augmentations\n",
    "                _img = RandGaussianNoised(keys[0], prob=1.0, std=0.01)(infer_data)[keys[0]]\n",
    "                pred = inferer(_img.to(device), net)\n",
    "                preds = preds + pred\n",
    "                n = n + 1.0\n",
    "                for dims in [[2], [3]]:\n",
    "                    flip_pred = inferer(torch.flip(_img.to(device), dims=dims), net)\n",
    "                    pred = torch.flip(flip_pred, dims=dims)\n",
    "                    preds = preds + pred\n",
    "                    n = n + 1.0\n",
    "            preds = preds / n\n",
    "            preds = (preds.argmax(dim=1, keepdims=True)).float()\n",
    "            saver.save_batch(preds, infer_data[\"image_meta_dict\"])\n",
    "\n",
    "    # copy the saved segmentations into the required folder structure for submission\n",
    "    submission_dir = os.path.join(prediction_folder, \"to_submit\")\n",
    "    if not os.path.exists(submission_dir):\n",
    "        os.makedirs(submission_dir)\n",
    "    files = glob.glob(os.path.join(prediction_folder, \"volume*\", \"*.nii.gz\"))\n",
    "    for f in files:\n",
    "        new_name = os.path.basename(f)\n",
    "        new_name = new_name[len(\"volume-covid19-A-0\"):]\n",
    "        new_name = new_name[: -len(\"_ct_seg.nii.gz\")] + \".nii.gz\"\n",
    "        to_name = os.path.join(submission_dir, new_name)\n",
    "        shutil.copy(f, to_name)\n",
    "    logging.info(f\"predictions copied to {submission_dir}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Usage:\n",
    "    python run_net.py train --data_folder \"COVID-19-20_v2/Train\" # run the training pipeline\n",
    "    python run_net.py infer --data_folder \"COVID-19-20_v2/Validation\" # run the inference pipeline\n",
    "\"\"\"\n",
    "parser = argparse.ArgumentParser(description=\"Run a basic UNet segmentation baseline.\")\n",
    "parser.add_argument(\n",
    "#     \"mode\", metavar=\"mode\", default=\"train\", choices=(\"train\", \"infer\"), type=str, help=\"mode of workflow\"\n",
    "    \"mode\", metavar=\"mode\", default=\"train\", type=str, help=\"mode of workflow\"\n",
    ")\n",
    "parser.add_argument(\"--data_folder\", default=\"\", type=str, help=\"training data folder\")\n",
    "parser.add_argument(\"--model_folder\", default=\"runs\", type=str, help=\"model folder\")\n",
    "parser.add_argument(\"--batch_size\", default=32, type=int, help=\"model folder\")\n",
    "parser.add_argument(\"--num_workers\", default=8, type=int, help=\"model folder\")\n",
    "parser.add_argument(\"--preprocessing_workers\", default=8, type=int, help=\"model folder\")\n",
    "args, _ = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.data_folder = \"/mnt/DATA2/congvm/COVID-19-20_v2/Train\" \n",
    "args.model_folder = \"runs\"\n",
    "args.mode = 'train'\n",
    "args.batch_size = 16\n",
    "args.num_workers = 8\n",
    "args.preprocessing_workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monai.config.print_config()\n",
    "# monai.utils.set_determinism(seed=0)\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "# if args.mode == \"train\":\n",
    "#     data_folder = args.data_folder or os.path.join(\"COVID-19-20_v2\", \"Train\")\n",
    "#     train(data_folder=data_folder, \n",
    "#           model_folder=args.model_folder, \n",
    "#           num_workers=args.num_workers, \n",
    "#           batch_size=args.batch_size, \n",
    "#           preprocessing_workers=args.preprocessing_workers)\n",
    "# elif args.mode == \"infer\":\n",
    "#     data_folder = args.data_folder or os.path.join(\"COVID-19-20_v2\", \"Validation\")\n",
    "#     infer(data_folder=data_folder, model_folder=args.model_folder)\n",
    "# else:\n",
    "#     raise ValueError(\"Unknown mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = sorted(glob.glob(os.path.join(args.data_folder, \"*_ct.nii.gz\")))\n",
    "labels = sorted(glob.glob(os.path.join(args.data_folder, \"*_seg.nii.gz\")))\n",
    "logging.info(f\"training: image/label ({len(images)}) folder: {args.data_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load and cache transformed data: 100%|██████████| 10/10 [00:16<00:00,  1.66s/it]\n"
     ]
    }
   ],
   "source": [
    "amp = True  # auto. mixed precision\n",
    "keys = (\"image\", \"label\")\n",
    "\n",
    "train_frac, val_frac = 0.8, 0.2\n",
    "n_train = int(train_frac * len(images)) + 1\n",
    "n_val = min(len(images) - n_train, int(val_frac * len(images)))\n",
    "logging.info(f\"training: train {n_train} val {n_val}, folder: {args.data_folder}\")\n",
    "\n",
    "train_files = [{keys[0]: img, keys[1]: seg} for img, seg in zip(images[:n_train], labels[:n_train])]\n",
    "val_files = [{keys[0]: img, keys[1]: seg} for img, seg in zip(images[-n_val:], labels[-n_val:])]\n",
    "\n",
    "# create a training data loader\n",
    "logging.info(f\"batch size {args.batch_size}\")\n",
    "train_transforms = get_xforms(\"train\", keys)\n",
    "train_ds = monai.data.CacheDataset(data=train_files[:10], \n",
    "                                   transform=train_transforms, \n",
    "                                   num_workers=args.preprocessing_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = monai.data.DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicUNet features: (32, 32, 64, 128, 256, 32).\n"
     ]
    }
   ],
   "source": [
    "net = get_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 192, 192, 16])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = data['image'].to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['image', 'label', 'image_meta_dict', 'label_meta_dict'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = data['label'].to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = net(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In MONAI Version > 0.2, y_pred needs to be normalized and \n",
    "# binarized with sigmoid and thresholding, respectively\n",
    "y_sigmoid = torch.sigmoid(y)\n",
    "logit_thresh = 0.5\n",
    "y_sigmoid = (y_sigmoid >= logit_thresh).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from monai.metrics.meandice import MeanDice\n",
    "val_metric = monai.metrics.DiceMetric(\n",
    "    include_background=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0409], device='cuda:1'), tensor(3., device='cuda:1'))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metric(y_sigmoid, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai\n",
    "import torch.optim as optim\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ln = torch.nn.Linear(2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundingExponentialLR(optim.lr_scheduler.ExponentialLR):\n",
    "    \"\"\"Decays the learning rate of each parameter group by gamma every epoch.\n",
    "    When last_epoch=-1, sets initial lr as lr.\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        gamma (float): Multiplicative factor of learning rate decay.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, gamma, initial_lr=0.01, min_lr=0.001, last_epoch=-1):\n",
    "        self.min_lr = min_lr\n",
    "        super().__init__(optimizer=optimizer, gamma=gamma, last_epoch=-1)\n",
    "\n",
    "    def _compute_lr(self, base_lr):\n",
    "        if base_lr * self.gamma <= self.min_lr:\n",
    "            return self.min_lr\n",
    "        else:\n",
    "            return base_lr * self.gamma\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        if self.last_epoch == 0:\n",
    "            return self.base_lrs\n",
    "\n",
    "        return [self._compute_lr(group['lr']) for group in self.optimizer.param_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sche = BoundingExponentialLR(optim.Adam(model.parameters(), lr=0.01), gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.001]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_sche.step()\n",
    "lr_sche.get_last_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.handlers import LrScheduleHandler, ValidationHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LrScheduleHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_net import get_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description=\"Run a basic UNet segmentation baseline.\")\n",
    "    parser.add_argument(\"--data_folder\", default=\"\", type=str, help=\"training data folder\")\n",
    "    parser.add_argument(\"--model_folder\", default=\"runs\", type=str, help=\"model folder\")\n",
    "    parser.add_argument(\"--batch_size\", default=32, type=int, help=\"batch size\")\n",
    "    parser.add_argument(\"--num_workers\", default=8, type=int, help=\"num workers\")\n",
    "    parser.add_argument(\"--preprocessing_workers\", default=8, type=int, help=\"preprocessing workers\")\n",
    "    parser.add_argument(\"--opt\", default='adam', type=str, choices=(\"adam\", \"sgd\"), help=\"opt\")\n",
    "    parser.add_argument(\"--cache_rate\", default=0.5, type=float, help=\"cache rate\")\n",
    "    parser.add_argument(\"--momentum\", default=0.95, type=float, help=\"opt momentum\")\n",
    "    parser.add_argument(\"--lr\", default=0.01, type=float, help=\"learning rate\")\n",
    "    parser.add_argument(\"--gamma\", default=0.5, type=float, help=\"lr scheduler gamma\")\n",
    "    parser.add_argument(\"--max_epochs\", default=500, type=int, help=\"lr scheduler gamma\")\n",
    "            \n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_folder': '',\n",
       " 'model_folder': 'runs',\n",
       " 'batch_size': 32,\n",
       " 'num_workers': 8,\n",
       " 'preprocessing_workers': 8,\n",
       " 'opt': 'adam',\n",
       " 'cache_rate': 0.5,\n",
       " 'momentum': 0.95,\n",
       " 'lr': 0.01,\n",
       " 'gamma': 0.5,\n",
       " 'max_epochs': 500}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "def save_args_to_file(args, save_folder):\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"config_%d-%m-%Y_%H:%M:%S.yaml\")\n",
    "    path_to_save = os.path.join(save_folder, dt_string)\n",
    "    with open(path_to_save, 'w') as f:\n",
    "        yaml.dump(vars(args), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CheckpointSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir:str,\n",
    "# save_dict:Dict,\n",
    "# name:Union[str, NoneType]=None,\n",
    "# file_prefix:str='',\n",
    "# save_final:bool=False,\n",
    "# final_filename:Union[str, NoneType]=None,\n",
    "# save_key_metric:bool=False,\n",
    "# key_metric_name:Union[str, NoneType]=None,\n",
    "# key_metric_n_saved:int=1,\n",
    "# key_metric_filename:Union[str, NoneType]=None,\n",
    "# epoch_level:bool=True,\n",
    "# save_interval:int=0,\n",
    "# n_saved:Union[int, NoneType]=None,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CheckpointSaver(save_dir=args.model_folder, save_dict={'net': net, \n",
    "                                                       'optimizer': opt},\n",
    "                save_key_metric=True, key_metric_n_saved=3), \n",
    "key_metric_name={'val_mean_dice': val_mean_dice, 'train_mean_dice': val_mean_dice}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
