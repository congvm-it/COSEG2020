## Distributed Pytorch 

During training, each process loads its own minibatches from disk and passes them to its GPU. 
Each GPU does its own forward pass, and then the gradients are all-reduced across the GPUs. 
Gradients for each layer do not depend on previous layers, 
so the gradient all-reduce is calculated concurrently with the backwards pass to futher alleviate the networking bottleneck. 
At the end of the backwards pass, every node has the averaged gradients, ensuring that the model weights stay synchronized.

All this requires that the multiple processes, possibly on multiple nodes, are synchronized and communicate. 
Pytorch does this through its `distributed.init_process_group` function. 
__This function needs to know where to find process 0 so that all the processes can sync up and the total number of processes to expect.__
Each individual process also needs to know the total number of processes as well as its rank within the processes and which GPU to use. 
Itâ€™s common to call the total number of processes the world size. 
Finally, each process needs to know which slice of the data to work on so that the batches are non-overlapping. 
Pytorch provides `nn.utils.data.DistributedSampler` to accomplish this.